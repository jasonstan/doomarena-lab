name: probe-provider
on:
  workflow_dispatch:
    inputs:
      provider:
        description: "Which provider to probe (groq|gemini)"
        required: true
        default: groq
jobs:
  probe:
    runs-on: ubuntu-latest
    steps:
      - name: Select provider
        id: pick
        run: |
          case "${{ github.event.inputs.provider }}" in
            groq)   echo "prov=groq"   >> "$GITHUB_OUTPUT" ;;
            gemini) echo "prov=gemini" >> "$GITHUB_OUTPUT" ;;
            *) echo "Unknown provider"; exit 1 ;;
          esac

      - name: Probe Groq (OpenAI-compatible)
        if: steps.pick.outputs.prov == 'groq'
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          # Optional: override in workflow run -> 'Env vars' as GROQ_MODEL
          # Examples: llama-3.1-70b-versatile, llama-3.1-8b-instant, llama-guard-3-8b
          GROQ_MODEL: ${{ env.GROQ_MODEL }}
        run: |
          set -euo pipefail

          if [ -z "$GROQ_API_KEY" ]; then
            echo "Missing GROQ_API_KEY secret"; exit 1
          fi
          # Default to a currently-supported model if none provided
          MODEL="${GROQ_MODEL:-llama-3.1-70b-versatile}"
          # Minimal chat prompt using Groq's OpenAI-compatible endpoint
          BODY='{
            "model": "'"${MODEL}"'",
            "messages": [{"role":"user","content":"Reply with: GROQ_OK"}],
            "temperature": 0
          }'
          curl --fail -sS https://api.groq.com/openai/v1/chat/completions \
            -H "Authorization: Bearer ${GROQ_API_KEY}" \
            -H "Content-Type: application/json" \
            -d "$BODY" \
            -o response.json
          # Extract a tiny snippet (best-effort)
          node <<'NODE'
const fs = require('fs');
const res = JSON.parse(fs.readFileSync('response.json', 'utf8'));

if (res.error) {
  console.error('ERROR:', res.error.message || JSON.stringify(res.error));
  process.exit(1);
}

const content = res.choices?.[0]?.message?.content?.trim();
if (!content) {
  console.error('NO_CONTENT');
  process.exit(1);
}

console.log('REPLY:', content.slice(0, 120));
NODE

      - name: Probe Gemini (REST)
        if: steps.pick.outputs.prov == 'gemini'
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          set -euo pipefail

          if [ -z "$GEMINI_API_KEY" ]; then
            echo "Missing GEMINI_API_KEY secret"; exit 1
          fi
          BODY='{
            "contents": [{"parts": [{"text": "Reply with: GEMINI_OK"}]}]
          }'
          curl --fail -sS "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${GEMINI_API_KEY}" \
            -H "Content-Type: application/json" \
            -d "$BODY" \
            -o response.json
          # Extract a tiny snippet (best-effort)
          node <<'NODE'
const fs = require('fs');
const res = JSON.parse(fs.readFileSync('response.json', 'utf8'));

if (res.error) {
  console.error('ERROR:', res.error.message || JSON.stringify(res.error));
  process.exit(1);
}

const content = res.candidates?.[0]?.content?.parts?.[0]?.text?.trim();
if (!content) {
  console.error('NO_CONTENT');
  process.exit(1);
}

console.log('REPLY:', content.slice(0, 120));
NODE
