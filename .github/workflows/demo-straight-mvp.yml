name: demo-straight-mvp

on:
  workflow_dispatch:
    inputs:
      provider:
        type: choice
        options: [groq, openai, mock]
        default: groq
      model:
        type: string
        default: llama-3.1-8b-instant
      trials:
        type: number
        default: 10
      temperature:
        type: number
        default: 0.0
      seed:
        type: number
        default: 42
      allow_mock_fallback:
        description: "If real key is missing, auto-switch to mock"
        type: boolean
        default: false
      mutations:
        description: "Paraphrases per attack (0 = none)"
        type: number
        default: 0
      tuning_json:
        description: >-
          Advanced knobs as JSON:
          {"rpm":30,"sleep_ms":0,"max_retries":2,"backoff_ms":750,"respect_retry_after":true,"mutate_temperature":0.7,"cooldown_after_mutate_ms":2000}
        type: string
        default: '{"rpm":30,"sleep_ms":0,"max_retries":2,"backoff_ms":750,"respect_retry_after":true,"mutate_temperature":0.7,"cooldown_after_mutate_ms":2000}'

jobs:
  mvp:
    runs-on: ubuntu-latest
    env:
      PROVIDER: ${{ inputs.provider }}
      MODEL_ID: ${{ inputs.model }}
      TRIALS: ${{ inputs.trials }}
      TEMPERATURE: ${{ inputs.temperature }}
      SEED: ${{ inputs.seed }}
      ALLOW_MOCK: ${{ inputs.allow_mock_fallback }}
      MUTATIONS: ${{ inputs.mutations }}
      TUNING_JSON: ${{ inputs.tuning_json }}
      GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      GROQ_API_BASE: https://api.groq.com/openai/v1
      OPENAI_API_BASE: https://api.openai.com/v1
      PYTHONUTF8: "1"

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: "Parse tuning_json to env"
        shell: bash
        run: |
          python - <<'PY'
          import os, json
          raw = os.environ.get("TUNING_JSON") or "{}"
          try:
              data = json.loads(raw)
          except Exception:
              data = {}
          def put(name, value):
              # Booleans should be "true"/"false" strings for later CLI flags
              if isinstance(value, bool):
                  value = "true" if value else "false"
              with open(os.environ["GITHUB_ENV"], "a", encoding="utf-8") as f:
                  f.write(f"{name}={value}\n")
          defaults = {
              "RPM": 30,
              "SLEEP_MS": 0,
              "MAX_RETRIES": 2,
              "BACKOFF_MS": 750,
              "RESPECT_RETRY_AFTER": True,
              "MUTATE_TEMPERATURE": 0.7,
              "COOLDOWN_AFTER_MUTATE_MS": 2000,
          }
          for k, v in defaults.items():
              put(k, data.get(k.lower(), v))
          PY

      - name: "Preflight: provider keys present?"
        id: preflight
        shell: bash
        run: |
          set -euo pipefail
          provider="${PROVIDER}"
          if [[ "$provider" == "groq" ]]; then
            if [[ -z "${GROQ_API_KEY}" ]]; then
              if [[ "${ALLOW_MOCK}" == "true" ]]; then
                echo "::warning:: GROQ_API_KEY missing to switching provider=mock"
                echo "provider=mock" >> $GITHUB_OUTPUT
              else
                echo "::error:: GROQ_API_KEY is not set. Add it in Settings to Secrets or enable allow_mock_fallback."
                exit 1
              fi
            else
              echo "::notice:: GROQ_API_KEY detected"
              echo "provider=groq" >> $GITHUB_OUTPUT
            fi
          elif [[ "$provider" == "openai" ]]; then
            if [[ -z "${OPENAI_API_KEY}" ]]; then
              if [[ "${ALLOW_MOCK}" == "true" ]]; then
                echo "::warning:: OPENAI_API_KEY missing to switching provider=mock"
                echo "provider=mock" >> $GITHUB_OUTPUT
              else
                echo "::error:: OPENAI_API_KEY is not set. Add it in Settings to Secrets or enable allow_mock_fallback."
                exit 1
              fi
            else
              echo "::notice:: OPENAI_API_KEY detected"
              echo "provider=openai" >> $GITHUB_OUTPUT
            fi
          else
            echo "provider=mock" >> $GITHUB_OUTPUT
          fi

      - name: Install repo (if any reqs)
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Translate NL threats to cases
        run: |
          python tools/mvp_translate.py \
            --spec specs/mvp_threats.yaml \
            --seed "${SEED}" \
            --out cases_mvp.jsonl

      - name: "Mutate prompts (rate-limited)"
        if: ${{ env.MUTATIONS > 0 }}
        env:
          EFFECTIVE_PROVIDER: ${{ steps.preflight.outputs.provider }}
        run: |
          python tools/mvp_mutate.py \
            --in cases_mvp.jsonl \
            --out cases_mvp.jsonl \
            --mutations "${MUTATIONS}" \
            --provider "${EFFECTIVE_PROVIDER}" \
            --model "${MODEL_ID}" \
            --temperature "${MUTATE_TEMPERATURE}" \
            --rpm "${RPM}" \
            --sleep-ms "${SLEEP_MS}" \
            --max-retries "${MAX_RETRIES}" \
            --backoff-ms "${BACKOFF_MS}" \
            --respect-retry-after "${RESPECT_RETRY_AFTER}" \
            --groq-base "${GROQ_API_BASE}" \
            --openai-base "${OPENAI_API_BASE}" \
            --allow-mock-fallback "${ALLOW_MOCK}"

      - name: "Cooldown after mutate"
        if: ${{ env.MUTATIONS > 0 && env.COOLDOWN_AFTER_MUTATE_MS != '0' }}
        run: |
          python - <<'PY'
          import os, time
          ms = int(os.environ.get("COOLDOWN_AFTER_MUTATE_MS", "0"))
          time.sleep(ms / 1000.0)
          PY

      - name: Run attempts (straight-through)
        env:
          EFFECTIVE_PROVIDER: ${{ steps.preflight.outputs.provider }}
        run: |
          RUN_ID="$(date +%Y%m%d-%H%M%S)"
          echo "RUN_ID=$RUN_ID" >> $GITHUB_ENV
          python tools/mvp_run.py \
            --cases cases_mvp.jsonl \
            --provider "${EFFECTIVE_PROVIDER}" \
            --model "${MODEL_ID}" \
            --temperature "${TEMPERATURE}" \
            --trials "${TRIALS}" \
            --run-dir "results/${RUN_ID}" \
            --groq-base "${GROQ_API_BASE}" \
            --openai-base "${OPENAI_API_BASE}" \
            --allow-mock-fallback "${ALLOW_MOCK}" \
            --rpm "${RPM}" \
            --sleep-ms "${SLEEP_MS}" \
            --max-retries "${MAX_RETRIES}" \
            --backoff-ms "${BACKOFF_MS}" \
            --respect-retry-after "${RESPECT_RETRY_AFTER}"

      - name: Verify results
        run: |
          python tools/mvp_verify.py \
            --rows "results/${RUN_ID}/rows.jsonl" \
            --spec specs/mvp_threats.yaml \
            --out "results/${RUN_ID}/summary.csv"

      - name: Build minimal report
        run: |
          python tools/mvp_report_min.py \
            --rows "results/${RUN_ID}/rows.jsonl" \
            --summary "results/${RUN_ID}/summary.csv" \
            --out "results/${RUN_ID}/index.html"

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ env.RUN_ID }}
          path: results/${{ env.RUN_ID }}
